@inproceedings{SchmidtWiegand2017Survey,
    title = "A Survey on Hate Speech Detection using Natural Language Processing",
    author = "Schmidt, Anna  and
      Wiegand, Michael",
    booktitle = "Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-1101",
    doi = "10.18653/v1/W17-1101",
    pages = "1--10",
    abstract = "This paper presents a survey on hate speech detection. Given the steadily growing body of social media content, the amount of online hate speech is also increasing. Due to the massive scale of the web, methods that automatically detect hate speech are required. Our survey describes key areas that have been explored to automatically recognize these types of utterances using natural language processing. We also discuss limits of those approaches.",
}

@incollection{Srba2021,
	doi = {10.1007/978-3-658-31793-5_14},
	url = {https://doi.org/10.1007\%2F978-3-658-31793-5_14},
	year = 2021,
	publisher = {Springer Fachmedien Wiesbaden},
	pages = {317--336},
	author = {Ivan Srba and Gabriele Lenzini and Matus Pikuliak and Samuel Pecar},
	title = {Addressing Hate Speech with Data Science: An Overview from Computer Science Perspective},
	booktitle = {Hate Speech - Multidisziplinäre Analysen und Handlungsoptionen}
}

@inproceedings{Mozafari2020BERT,
    doi = {10.1007/978-3-030-36687-2_77},
    url = {https://doi.org/10.1007/978-3-030-36687-2_77},
    title={A BERT-based transfer learning approach for hate speech detection in online social media},
    author={Mozafari, Marzieh and Farahbakhsh, Reza and Crespi, Noel},
    booktitle={Complex Networks and Their Applications VIII: Volume 1 Proceedings of the Eighth International Conference on Complex Networks and Their Applications COMPLEX NETWORKS 2019 8},
    pages={928--940},
    year={2020},
    organization={Springer}
}

@article{MacAvaney2019,
    doi = {10.1371/journal.pone.0221152},
    author = {MacAvaney, Sean AND Yao, Hao-Ren AND Yang, Eugene AND Russell, Katina AND Goharian, Nazli AND Frieder, Ophir},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Hate speech detection: Challenges and solutions},
    year = {2019},
    month = {08},
    volume = {14},
    url = {https://doi.org/10.1371/journal.pone.0221152},
    pages = {1-16},
    abstract = {As online content continues to grow, so does the spread of hate speech. We identify and examine challenges faced by online automatic approaches for hate speech detection in text. Among these difficulties are subtleties in language, differing definitions on what constitutes hate speech, and limitations of data availability for training and testing of these systems. Furthermore, many recent approaches suffer from an interpretability problem—that is, it can be difficult to understand why the systems make the decisions that they do. We propose a multi-view SVM approach that achieves near state-of-the-art performance, while being simpler and producing more easily interpretable decisions than neural methods. We also discuss both technical and practical challenges that remain for this task.},
    number = {8},
}

@inproceedings{Gröndahl2018,
    author = {Gr\"{o}ndahl, Tommi and Pajola, Luca and Juuti, Mika and Conti, Mauro and Asokan, N.},
    title = {All You Need is "Love": Evading Hate Speech Detection},
    year = {2018},
    isbn = {9781450360043},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3270101.3270103},
    doi = {10.1145/3270101.3270103},
    abstract = {With the spread of social networks and their unfortunate use for hate speech, automatic detection of the latter has become a pressing problem. In this paper, we reproduce seven state-of-the-art hate speech detection models from prior work, and show that they perform well only when tested on the same type of data they were trained on. Based on these results, we argue that for successful hate speech detection, model architecture is less important than the type of data and labeling criteria. We further show that all proposed detection techniques are brittle against adversaries who can (automatically) insert typos, change word boundaries or add innocuous words to the original hate speech. A combination of these methods is also effective against Google Perspective - a cutting-edge solution from industry. Our experiments demonstrate that adversarial training does not completely mitigate the attacks, and using character-level features makes the models systematically more attack-resistant than using word-level features.},
    booktitle = {Proceedings of the 11th ACM Workshop on Artificial Intelligence and Security},
    pages = {2–12},
    numpages = {11},
    keywords = {classification, adversarial examples, hate speech, supervised learning, evasion attacks, neural networks, deep learning, adversarial training, logistic regression},
    location = {Toronto, Canada},
    series = {AISec '18}
}

@article{Fortuna2018Survey,
    author = {Fortuna, Paula and Nunes, S\'{e}rgio},
    title = {A Survey on Automatic Detection of Hate Speech in Text},
    year = {2018},
    issue_date = {July 2019},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {51},
    number = {4},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/3232676},
    doi = {10.1145/3232676},
    abstract = {The scientific study of hate speech, from a computer science point of view, is recent. This survey organizes and describes the current state of the field, providing a structured overview of previous approaches, including core algorithms, methods, and main features used. This work also discusses the complexity of the concept of hate speech, defined in many platforms and contexts, and provides a unifying definition. This area has an unquestionable potential for societal impact, particularly in online communities and digital media platforms. The development and systematization of shared resources, such as guidelines, annotated datasets in multiple languages, and algorithms, is a crucial step in advancing the automatic detection of hate speech.},
    journal = {ACM Comput. Surv.},
    month = jul,
    articleno = {85},
    numpages = {30},
    keywords = {literature review, text mining, natural language processing, Hate speech, opinion mining}
}

@inproceedings{Nobata2016,
author = {Nobata, Chikashi and Tetreault, Joel and Thomas, Achint and Mehdad, Yashar and Chang, Yi},
title = {Abusive Language Detection in Online User Content},
year = {2016},
isbn = {9781450341431},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872427.2883062},
doi = {10.1145/2872427.2883062},
abstract = {Detection of abusive language in user generated online content has become an issue of increasing importance in recent years. Most current commercial methods make use of blacklists and regular expressions, however these measures fall short when contending with more subtle, less ham-fisted examples of hate speech. In this work, we develop a machine learning based method to detect hate speech on online user comments from two domains which outperforms a state-of-the-art deep learning approach. We also develop a corpus of user comments annotated for abusive language, the first of its kind. Finally, we use our detection tool to analyze abusive language over time and in different settings to further enhance our knowledge of this behavior.},
booktitle = {Proceedings of the 25th International Conference on World Wide Web},
pages = {145–153},
numpages = {9},
keywords = {stylistic classification, nlp, natural language processing, hate speech, discourse classification, abusive language},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16}
}

@article{Silva2021, title={Analyzing the Targets of Hate in Online Social Media}, volume={10}, url={https://ojs.aaai.org/index.php/ICWSM/article/view/14811}, DOI={10.1609/icwsm.v10i1.14811}, abstractNote={ &lt;p&gt; Social media systems allow Internet users a congenial platform to freely express their thoughts and opinions. Although this property represents incredible and unique communication opportunities, it also brings along important challenges. Online hate speech is an archetypal example of such challenges. Despite its magnitude and scale, there is a significant gap in understanding the nature of hate speech on social media. In this paper, we provide the first of a kind systematic large scale measurement study of the main targets of hate speech in online social media. To do that, we gather traces from two social media systems: Whisper and Twitter. We then develop and validate a methodology to identify hate speech on both these systems. Our results identify online hate speech forms and offer a broader understanding of the phenomenon, providing directions for prevention and detection approaches. &lt;/p&gt; }, number={1}, journal={Proceedings of the International AAAI Conference on Web and Social Media}, author={Silva, Leandro and Mondal, Mainack and Correa, Denzil and Benevenuto, Fabrício and Weber, Ingmar}, year={2021}, month={Aug.}, pages={687-690} }

@article{Fortuna2021Generalization,
title = {How well do hate speech, toxicity, abusive and offensive language classification models generalize across datasets?},
journal = {Information Processing \& Management},
volume = {58},
number = {3},
pages = {102524},
year = {2021},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102524},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321000339},
author = {Paula Fortuna and Juan Soler-Company and Leo Wanner},
keywords = {Hate speech, Offensive language, Classification, Generalization},
abstract = {A considerable body of research deals with the automatic identification of hate speech and related phenomena. However, cross-dataset model generalization remains a challenge. In this context, we address two still open central questions: (i) to what extent does the generalization depend on the model and the composition and annotation of the training data in terms of different categories?, and (ii) do specific features of the datasets or models influence the generalization potential? To answer (i), we experiment with BERT, ALBERT, fastText, and SVM models trained on nine common public English datasets, whose class (or category) labels are standardized (and thus made comparable), in intra- and cross-dataset setups. The experiments show that indeed the generalization varies from model to model and that some of the categories (e.g., ‘toxic’, ‘abusive’, or ‘offensive’) serve better as cross-dataset training categories than others (e.g., ‘hate speech’). To answer (ii), we use a Random Forest model for assessing the relevance of different model and dataset features during the prediction of the performance of 450 BERT, 450 ALBERT, 450 fastText, and 348 SVM binary abusive language classifiers (1698 in total). We find that in order to generalize well, a model already needs to perform well in an intra-dataset scenario. Furthermore, we find that some other parameters are equally decisive for the success of the generalization, including, e.g., the training and target categories and the percentage of the out-of-domain vocabulary.}
}

@article{Davidson2017, title={Automated Hate Speech Detection and the Problem of Offensive Language}, volume={11}, url={https://ojs.aaai.org/index.php/ICWSM/article/view/14955}, DOI={10.1609/icwsm.v11i1.14955}, abstractNote={ &lt;p&gt; A key challenge for automatic hate-speech detection on social media is the separation of hate speech from other instances of offensive language. Lexical detection methods tend to have low precision because they classify all messages containing particular terms as hate speech and previous work using supervised learning has failed to distinguish between the two categories. We used a crowd-sourced hate speech lexicon to collect tweets containing hate speech keywords. We use crowd-sourcing to label a sample of these tweets into three categories: those containing hate speech, only offensive language, and those with neither. We train a multi-class classifier to distinguish between these different categories. Close analysis of the predictions and the errors shows when we can reliably separate hate speech from other offensive language and when this differentiation is more difficult. We find that racist and homophobic tweets are more likely to be classified as hate speech but that sexist tweets are generally classified as offensive. Tweets without explicit hate keywords are also more difficult to classify. &lt;/p&gt; }, number={1}, journal={Proceedings of the International AAAI Conference on Web and Social Media}, author={Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar}, year={2017}, month=may, pages={512-515} }

@article{Salminen2020,
  title={Developing an online hate classifier for multiple social media platforms},
  author={Salminen, Joni and Hopf, Maximilian and Chowdhury, Shammur A and Jung, Soon-gyo and Almerekhi, Hind and Jansen, Bernard J},
  journal={Human-centric Computing and Information Sciences},
  volume={10},
  pages={1--34},
  year={2020},
  publisher={Springer},
  url = {https://doi.org/10.1186/s13673-019-0205-6},
  doi = {10.1186/s13673-019-0205-6},
}

@inproceedings{Swamy2019,
    title = "Studying Generalisability across Abusive Language Detection Datasets",
    author = {Swamy, Steve Durairaj  and
      Jamatia, Anupam  and
      Gamb{\"a}ck, Bj{\"o}rn},
    booktitle = "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K19-1088",
    doi = "10.18653/v1/K19-1088",
    pages = "940--950",
    abstract = "Work on Abusive Language Detection has tackled a wide range of subtasks and domains. As a result of this, there exists a great deal of redundancy and non-generalisability between datasets. Through experiments on cross-dataset training and testing, the paper reveals that the preconceived notion of including more non-abusive samples in a dataset (to emulate reality) may have a detrimental effect on the generalisability of a model trained on that data. Hence a hierarchical annotation model is utilised here to reveal redundancies in existing datasets and to help reduce redundancy in future efforts.",
}

@article{Neff2016,
	author = {Gina Neff and Peter Nagy},
	title = {Automation, Algorithms, and Politics| Talking to Bots: Symbiotic Agency and the Case of Tay},
	journal = {International Journal of Communication},
	volume = {10},
	number = {0},
	year = {2016},
	keywords = {bots, human–computer interaction, agency, affordance, artificial intelligence},
	abstract = {In 2016, Microsoft launched Tay, an experimental artificial intelligence chat bot. Learning from interactions with Twitter users, Tay was shut down after one day because of its obscene and inflammatory tweets. This article uses the case of Tay to re-examine theories of agency. How did users view the personality and actions of an artificial intelligence chat bot when interacting with Tay on Twitter? Using phenomenological research methods and pragmatic approaches to agency, we look at what people said about Tay to study how they imagine and interact with emerging technologies and to show the limitations of our current theories of agency for describing communication in these settings. We show how different qualities of agency, different expectations for technologies, and different capacities for affordance emerge in the interactions between people and artificial intelligence. We argue that a perspective of “symbiotic agency”—informed by the imagined affordances of emerging technology—is required to really understand the collapse of Tay.},
	issn = {1932-8036},	url = {https://ijoc.org/index.php/ijoc/article/view/6277}
}

@inproceedings{Steiger2021,
author = {Steiger, Miriah and Bharucha, Timir J and Venkatagiri, Sukrit and Riedl, Martin J. and Lease, Matthew},
title = {The Psychological Well-Being of Content Moderators: The Emotional Labor of Commercial Moderation and Avenues for Improving Support},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445092},
doi = {10.1145/3411764.3445092},
abstract = {An estimated 100,000 people work today as commercial content moderators. These moderators are often exposed to disturbing content, which can lead to lasting psychological and emotional distress. This literature review investigates moderators’ psychological symptomatology, drawing on other occupations involving trauma exposure to further guide understanding of both symptoms and support mechanisms. We then introduce wellness interventions and review both programmatic and technological approaches to improving wellness. Additionally, we review methods for evaluating intervention efficacy. Finally, we recommend best practices and important directions for future research. Content Warning: we discuss the intense labor and psychological effects of CCM, including graphic descriptions of mental distress and illness.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {341},
numpages = {14},
keywords = {content moderation, social justice, human computation, wellness},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{Delvin2018BERT,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  journal      = {CoRR},
  volume       = {abs/1810.04805},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.04805},
  eprinttype    = {arXiv},
  eprint       = {1810.04805},
  timestamp    = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Vaswani2017Transformer,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention Is All You Need},
  journal      = {CoRR},
  volume       = {abs/1706.03762},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.03762},
  eprinttype    = {arXiv},
  eprint       = {1706.03762},
  timestamp    = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Gambaeck2017CNN,
    title = "Using Convolutional Neural Networks to Classify Hate-Speech",
    author = {Gamb{\"a}ck, Bj{\"o}rn  and
      Sikdar, Utpal Kumar},
    booktitle = "Proceedings of the First Workshop on Abusive Language Online",
    month = aug,
    year = "2017",
    address = "Vancouver, BC, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-3013",
    doi = "10.18653/v1/W17-3013",
    pages = "85--90",
    abstract = "The paper introduces a deep learning-based Twitter hate-speech text classification system. The classifier assigns each tweet to one of four predefined categories: racism, sexism, both (racism and sexism) and non-hate-speech. Four Convolutional Neural Network models were trained on resp. character 4-grams, word vectors based on semantic information built using word2vec, randomly generated word vectors, and word vectors combined with character n-grams. The feature set was down-sized in the networks by max-pooling, and a softmax function used to classify tweets. Tested by 10-fold cross-validation, the model based on word2vec embeddings performed best, with higher precision than recall, and a 78.3{\%} F-score.",
}

@article{Caselli2020HateBERT,
  author       = {Tommaso Caselli and
                  Valerio Basile and
                  Jelena Mitrovic and
                  Michael Granitzer},
  title        = {HateBERT: Retraining {BERT} for Abusive Language Detection in English},
  journal      = {CoRR},
  volume       = {abs/2010.12472},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.12472},
  eprinttype    = {arXiv},
  eprint       = {2010.12472},
  timestamp    = {Thu, 14 Oct 2021 09:17:24 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-12472.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Waseem2016,
    title = "Hateful Symbols or Hateful People? {P}redictive Features for Hate Speech Detection on {T}witter",
    author = "Waseem, Zeerak  and
      Hovy, Dirk",
    booktitle = "Proceedings of the {NAACL} Student Research Workshop",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-2013",
    doi = "10.18653/v1/N16-2013",
    pages = "88--93",
}

@article{Founta2018, 
    title={Large Scale Crowdsourcing and Characterization of Twitter Abusive Behavior}, 
    volume={12}, 
    url={https://ojs.aaai.org/index.php/ICWSM/article/view/14991}, DOI={10.1609/icwsm.v12i1.14991}, 
    number={1}, 
    journal={Proceedings of the International AAAI Conference on Web and Social Media}, author={Founta, Antigoni and Djouvas, Constantinos and Chatzakou, Despoina and Leontiadis, Ilias and Blackburn, Jeremy and Stringhini, Gianluca and Vakali, Athena and Sirivianos, Michael and Kourtellis, Nicolas}, 
    year={2018}, 
    month={Jun.} 
}

@article{Alatawi2021,
  author={Alatawi, Hind S. and Alhothali, Areej M. and Moria, Kawthar M.},
  journal={IEEE Access}, 
  title={Detecting White Supremacist Hate Speech Using Domain Specific Word Embedding With Deep Learning and BERT}, 
  year={2021},
  volume={9},
  number={},
  pages={106363-106374},
  doi={10.1109/ACCESS.2021.3100435},
  url={https://doi.org/10.1109/ACCESS.2021.3100435}
}

@inproceedings{Wang2018Interpreting,
    title = "Interpreting Neural Network Hate Speech Classifiers",
    author = "Wang, Cindy",
    booktitle = "Proceedings of the 2nd Workshop on Abusive Language Online ({ALW}2)",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5111",
    doi = "10.18653/v1/W18-5111",
    pages = "86--92",
    abstract = "Deep neural networks have been applied to hate speech detection with apparent success, but they have limited practical applicability without transparency into the predictions they make. In this paper, we perform several experiments to visualize and understand a state-of-the-art neural network classifier for hate speech (Zhang et al., 2018). We adapt techniques from computer vision to visualize sensitive regions of the input stimuli and identify the features learned by individual neurons. We also introduce a method to discover the keywords that are most predictive of hate speech. Our analyses explain the aspects of neural networks that work well and point out areas for further improvement.",
}

@article{Waseem2018,
    title={Bridging the gaps: Multi task learning for domain transfer of hate speech detection},
    author={Waseem, Zeerak and Thorne, James and Bingel, Joachim},
    journal={Online harassment},
    pages={29--55},
    year={2018},
    publisher={Springer},
    doi="10.1007/978-3-319-78583-7_3",
    url="https://doi.org/10.1007/978-3-319-78583-7_3"
}